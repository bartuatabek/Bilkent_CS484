{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as tdata\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "import skimage.io\n",
    "from tqdm import tqdm\n",
    "import selective_search as ss\n",
    "from torchvision.transforms import functional as F\n",
    "from natsort import natsorted\n",
    "import itertools\n",
    "import ntpath\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(image):\n",
    "    image = image.convert('RGB')\n",
    "    image = np.asarray(image)\n",
    "    row = image.shape[0]\n",
    "    col = image.shape[1]\n",
    "    if(row > col):\n",
    "        right = math.ceil((row-col)/2)\n",
    "        left = math.floor((row-col)/2)\n",
    "        image = np.pad(image, ((0,0),(left,right),(0,0)), 'constant')\n",
    "    else:\n",
    "        top = math.ceil((col-row)/2)\n",
    "        bottom = math.floor((col-row)/2)\n",
    "        image = np.pad(image, ((top,bottom),(0,0),(0,0)), 'constant')\n",
    "    image = np.array(Image.fromarray(image).resize((224,224)))\n",
    "    image = image / 255\n",
    "    image[:,:,0] = (image[:,:,0] - 0.485) / 0.229\n",
    "    image[:,:,1] = (image[:,:,1] - 0.456) / 0.224\n",
    "    image[:,:,2] = (image[:,:,2] -  0.406) / 0.225\n",
    "    image = image.astype('float32')\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and preprocess the data\n",
    "train_images = []\n",
    "train_labels = []\n",
    "filenames = []\n",
    "for idx,dirname in enumerate(glob.glob('cs484_project_data/train/*')):\n",
    "    filenames.append(ntpath.basename(dirname))\n",
    "    for filename in glob.glob(dirname +\"/*\"):\n",
    "        image = Image.open(filename)\n",
    "        image = preprocess_data(image)\n",
    "        train_images.append(image)\n",
    "        train_labels.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_truth_data = pd.read_csv(\"cs484_project_data/test/bounding_box.txt\", header=None)\n",
    "test_images = []\n",
    "test_labels = []\n",
    "test_image_fnames= glob.glob('cs484_project_data/test/images/*')\n",
    "test_image_fnames = natsorted(test_image_fnames)\n",
    "for i,filename in enumerate(test_image_fnames):\n",
    "    test_labels.append(filenames.index(test_truth_data[0][i]))\n",
    "    image = Image.open(filename)\n",
    "    test_images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 9/100 [01:38<19:54, 13.13s/it]"
     ]
    }
   ],
   "source": [
    "box_proposal_list = []\n",
    "for image in tqdm(test_images):\n",
    "    box_proposal_list.append(ss.selective_search(image, mode='single', random=False))\n",
    "box_proposal_list = np.array(box_proposal_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2048, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out) \n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(ground_truth, predictions, labels):\n",
    "    label_no = len(labels)\n",
    "    conf_mat = np.zeros((label_no, label_no),dtype=int);\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        true_val = ground_truth[i]\n",
    "        prediction = predictions[i]\n",
    "        conf_mat[true_val][prediction] = conf_mat[true_val][prediction] + 1\n",
    "    return conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_mat(cm, classes):\n",
    "    \n",
    "    for i in range(len(classes)):\n",
    "        classes[i] = str(classes[i])\n",
    "        \n",
    "    plt.imshow(cm, cmap=plt.cm.Reds)\n",
    "    plt.title(\"Confusion Matrix for Class Predictions\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    max_val = cm.max()/2\n",
    "\n",
    "    for i, j in itertools.product(range(len(classes)), range(len(classes))):\n",
    "        curr =  cm[i, j]      \n",
    "        \n",
    "        if curr > max_val:\n",
    "            txt_col = \"white\"\n",
    "        else:\n",
    "            txt_col = \"black\"\n",
    "                                  \n",
    "        plt.text(j, i, format(curr, 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color= txt_col)\n",
    "\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(cm, labels):\n",
    "    tp_arr = np.zeros(len(labels))\n",
    "    fp_arr = np.zeros(len(labels))\n",
    "    fn_arr = np.zeros(len(labels))\n",
    "    tn_arr = np.zeros(len(labels))\n",
    "    \n",
    "    sum_false_pos = cm.sum(axis=0)\n",
    "    sum_false_neg = cm.sum(axis=1)\n",
    "    \n",
    "    total = sum_false_neg.sum()\n",
    "    prec = 0\n",
    "    recall = 0\n",
    "\n",
    "    for label in labels:\n",
    "        prec = 0\n",
    "        recall = 0\n",
    "        label = int(label)\n",
    "        tp_arr[label] = cm[label][label]\n",
    "        fp_arr[label] = sum_false_pos[label] - tp_arr[label]\n",
    "        fn_arr[label] = sum_false_neg[label] - tp_arr[label]\n",
    "        tn_arr[label] = total - fp_arr[label] - fn_arr[label] - tp_arr[label]\n",
    "        \n",
    "        prec = tp_arr[label] / (tp_arr[label] + fp_arr[label])\n",
    "        recall = tp_arr[label] / (tp_arr[label] + fn_arr[label])\n",
    "        \n",
    "        print(\"Precision for class \" + str(label), prec)\n",
    "        print(\"Recall for class \" + str(label), recall)\n",
    "\n",
    "    acc = ((tp_arr.sum() ) *100) / total\n",
    "    print(\"Overall Accuracy \" , acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOrganizedIndexes(correct_num, incorrect_num, test_labels,overlap_ratio_arr):\n",
    "    total_num = incorrect_num + correct_num\n",
    "    pic_index = []\n",
    "    correct_indexes = np.where(np.array(overlap_ratio_arr) >=0.5)\n",
    "    incorrect_indexes = np.where(np.array(overlap_ratio_arr) < 0.5)\n",
    "    for class_c in range(10):\n",
    "        incorrect_num_in = incorrect_num\n",
    "        correct_num_in = correct_num\n",
    "\n",
    "        curr_correct_num = 0\n",
    "        curr_incorrect_num = 0\n",
    "        curr_class_i = []\n",
    "        class_c_correct = [index for index in correct_indexes[0] if test_labels[index] == class_c]\n",
    "        class_c_incorrect = [index for index in incorrect_indexes[0] if test_labels[index] == class_c]\n",
    "        if correct_num > len(class_c_correct):\n",
    "            incorrect_num_in  = incorrect_num + correct_num - len(class_c_correct)\n",
    "\n",
    "        if incorrect_num > len(class_c_incorrect):\n",
    "            correct_num_in  = correct_num + incorrect_num - len(class_c_incorrect)\n",
    "\n",
    "        for index in class_c_correct:\n",
    "            if curr_correct_num == correct_num_in:\n",
    "                break\n",
    "            curr_correct_num = curr_correct_num + 1\n",
    "            curr_class_i.append((index,True))\n",
    "\n",
    "        for index in class_c_incorrect:\n",
    "            if curr_incorrect_num == incorrect_num_in:\n",
    "                break\n",
    "            curr_incorrect_num = curr_incorrect_num + 1\n",
    "            curr_class_i.append((index,False))\n",
    "        pic_index.append(curr_class_i)\n",
    "    return pic_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_func_to_rule_them_all(batch_s, hidden_layer_size, learning_rate, c_optimizer):\n",
    "    resnet50 = models.resnet50(pretrained=True)\n",
    "    feature_extractor = torch.nn.Sequential(*list(resnet50.children())[:-1]).eval().to(device)\n",
    "\n",
    "    feature_arr = []\n",
    "    for img in tqdm(train_images):\n",
    "        # we append an augmented dimension to indicate batch_size, which is one\n",
    "        image = np.reshape(img, [1, 224, 224, 3])\n",
    "        # model takes as input images of size [batch_size, 3, im_height, im_width]\n",
    "        image = np.transpose(image, [0, 3, 1, 2])\n",
    "        # convert the Numpy image to torch.FloatTensor\n",
    "        image = torch.from_numpy(image).to(device)\n",
    "        # extract features\n",
    "        feature_vector = feature_extractor(image)\n",
    "        # convert the features of type torch.FloatTensor to a Numpy array\n",
    "        # so that you can either work with them within the sklearn environment # or save them as .mat files\n",
    "        feature_vector = feature_vector.cpu().detach().numpy()\n",
    "        #feature_vector = feature_vector / np.linalg.norm(feature_vector)\n",
    "        feature_arr.append(feature_vector.squeeze())\n",
    "\n",
    "    feature_arr = np.reshape(feature_arr, [398,2048])\n",
    "    tensor_x = torch.from_numpy(feature_arr)\n",
    "    tensor_y = torch.from_numpy(np.array(train_labels)).type(\"torch.LongTensor\")\n",
    "\n",
    "    train_data = tdata.TensorDataset(tensor_x, tensor_y)\n",
    "    train_dataloader = tdata.DataLoader(train_data, batch_size=batch_s, shuffle=True)\n",
    "    model = FeedForward(hidden_layer_size).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if c_optimizer == 1:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(10):  # loop over the dataset multiple times    \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for i, (features,labels) in enumerate(train_dataloader):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(features)\n",
    "\n",
    "            #print(label)\n",
    "            loss = criterion(outputs.to(device), labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item() * features.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_data)\n",
    "        epoch_acc = running_corrects.double() / len(train_data)\n",
    "        print('loss: {:.4f}, acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "    print('Finished Training')\n",
    "\n",
    "    #Unused section only use to filter boxes to improve speed \n",
    "    global box_proposal_list \n",
    "#     box_proposal_list_filtered = []\n",
    "#     for boxes in box_proposal_list:\n",
    "#          box_proposal_list_filtered.append(ss.box_filter(boxes, min_size=100))\n",
    "#     box_proposal_list = box_proposal_list_filtered\n",
    "    # Above will be commented\n",
    "    \n",
    "    from operator import itemgetter\n",
    "    images_box_class = []\n",
    "    for image,box_proposals in tqdm(zip(test_images,box_proposal_list),total=len(test_images)):\n",
    "        max_preds = []\n",
    "        for box in box_proposals:\n",
    "            cropped_img = image.crop(box)\n",
    "            cropped_img = preprocess_data(cropped_img)\n",
    "            cropped_img = np.reshape(cropped_img, [1, 224, 224, 3])\n",
    "            cropped_img = np.transpose(cropped_img, [0, 3, 1, 2])\n",
    "            cropped_img_tensor = torch.from_numpy(cropped_img).to(device)\n",
    "            feature = feature_extractor(cropped_img_tensor)\n",
    "            pred_logits_tensor = model(feature.view(1,2048))\n",
    "            pred_probs = nn.functional.softmax(pred_logits_tensor, dim=1).cpu().data.numpy()\n",
    "            max_preds.append({\"class\": np.argmax(pred_probs[0]),\"prob\": max(pred_probs[0]), \"box\": box})\n",
    "        max_item = max(max_preds, key=itemgetter('prob'))\n",
    "        images_box_class.append({\"class\": max_item['class'],\"box\": max_item['box'], \"prob\": max_item['prob']})\n",
    "\n",
    "    classes_c=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    preds = [item['class'] for item in images_box_class]\n",
    "    cnf_matrix = get_confusion_matrix(test_labels, preds, classes_c)\n",
    "\n",
    "    plt.figure()\n",
    "    plot_conf_mat(cnf_matrix, classes_c)\n",
    "\n",
    "    get_statistics(cnf_matrix, classes_c)\n",
    "\n",
    "    overlap_ratio_arr = []\n",
    "    true_box_arr = []\n",
    "    for pred, (index,truth) in zip(images_box_class, test_truth_data.iterrows()):\n",
    "        pred_box = pred['box']\n",
    "        min_x = max(pred_box[0],truth[1])\n",
    "        min_y = max(pred_box[1],truth[2])\n",
    "        max_x = min(pred_box[2],truth[3])\n",
    "        max_y = min(pred_box[3],truth[4])\n",
    "        true_box_arr.append([truth[1],truth[2],truth[3],truth[4]])\n",
    "        pred_area = (pred_box[2] - pred_box[0] + 1)*(pred_box[3] - pred_box[1] + 1) \n",
    "        truth_area = (truth[3] - truth[1] + 1)*(truth[4] - truth[2] + 1)\n",
    "        intersection_area = 0\n",
    "\n",
    "        if(min_x < max_x and min_y < max_y):\n",
    "            intersection_area = (max_x - min_x + 1)*(max_y - min_y + 1)\n",
    "        overlap_ratio_arr.append(intersection_area/(pred_area+truth_area-intersection_area))\n",
    "\n",
    "    total_correct_pred = 0\n",
    "    for pred, test_label,overlap_ratio in zip(preds,test_labels,overlap_ratio_arr):\n",
    "        if (pred == test_label) and overlap_ratio >= 0.5:\n",
    "            total_correct_pred = total_correct_pred + 1\n",
    "\n",
    "    localization_acc = total_correct_pred/len(test_images)\n",
    "    print(\"Overall Localization Accuracy \" , localization_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_s, hidden_layer_size, learning_rate, c_optimizer\n",
    "batch_sizes = [4, 8, 16, 32]\n",
    "hidden_layer_sizes = [100, 500, 1000, 1375]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "c_optimizers = [1,2] # 1 = SGD 2 = Adam\n",
    "\n",
    "for batch_size in tqdm(batch_sizes):\n",
    "    for hidden_layer_size in hidden_layer_sizes:\n",
    "        for learning_rate in learning_rates:\n",
    "            for c_optimizer in c_optimizers:\n",
    "                print(\"Batch Size:\", batch_size, \"Hidden Layer Size:\", hidden_layer_size, \"Learning Rate:\", learning_rate, \"Optimizer:\", \"SGD\" if c_optimizer == 1 else \"Adam\")\n",
    "                one_func_to_rule_them_all(batch_size, hidden_layer_size, learning_rate, c_optimizer)\n",
    "                print(\"----------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pic_index = getOrganizedIndexes(1, 1, test_labels,overlap_ratio_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for indexes in pic_index:\n",
    "#     fig, ax = plt.subplots(figsize=(6, 6))\n",
    "#     ax.imshow(test_images[indexes[0][0]])\n",
    "#     boxes = box_proposal_list[indexes[0][0]]\n",
    "#     boxes_filtered = ss.box_filter(boxes, min_size=100)\n",
    "#     cmap = plt.cm.get_cmap('hsv', len(boxes_filtered))\n",
    "#     for i,(x1, y1, x2, y2) in enumerate(boxes_filtered):\n",
    "#         bbox = mpatches.Rectangle(\n",
    "#             (x1, y1), (x2-x1), (y2-y1), fill=False, edgecolor=cmap(i), linewidth=1)\n",
    "#         ax.add_patch(bbox)\n",
    "        \n",
    "\n",
    "#     plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for class_c,indexes in enumerate(pic_index):\n",
    "#     for i in indexes:\n",
    "#         fig, ax = plt.subplots(figsize=(6, 6))\n",
    "#         ax.imshow(test_images[i[0]])\n",
    "#         img_bc = images_box_class[i[0]]\n",
    "#         box = img_bc['box']\n",
    "#         prob = img_bc['prob']\n",
    "\n",
    "#         x1, y1, x2, y2 = true_box_arr[i[0]]\n",
    "#         bbox_true = mpatches.Rectangle(\n",
    "#             (x1, y1), (x2-x1), (y2-y1), fill=False, edgecolor='green', linewidth=2)\n",
    "#         ax.add_patch(bbox_true)\n",
    "        \n",
    "#         x1, y1, x2, y2 = box\n",
    "#         bbox_pred = mpatches.Rectangle(\n",
    "#             (x1, y1), (x2-x1), (y2-y1), fill=False, edgecolor='yellow', linewidth=2)\n",
    "#         ax.add_patch(bbox_pred)\n",
    "#         plt.text(x1, y1-10,'Class [' + str(class_c) + ']: ' + '{:.1%}'.format(prob), color='yellow', fontsize=15)\n",
    "        \n",
    "#         ax.set_title('Correctly Localized: '+ str(i[1]), y=-0.09, fontsize=15)    \n",
    "#         plt.axis('off')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
